# Glances Code Human Preferences - Prompt Collection

This document contains 10 main prompts with 3 follow-up prompts each for the Glances system monitoring tool codebase. All prompts adhere to the Code Human Preferences CLI Edition instructions.

---

## Prompt Set 1: Plugin Configuration Validation

### Main Prompt (Turn 1)

Add input validation for plugin configuration values loaded from the config file. When plugins read their configuration settings through the Config class methods (get_int_value, get_float_value, get_bool_value), there's currently no validation that the values fall within acceptable ranges. This can lead to runtime errors or unexpected behavior when users provide invalid configuration values. Implement validation that ensures configuration values are within reasonable bounds for each plugin, with clear error messages when invalid values are detected.

### Follow-up Prompt (Turn 2)

The validation logic you added is good, but I noticed a few issues. First, the error messages should specify which configuration file was loaded so users know where to fix the problem - you can get this from `config.loaded_config_file()`. Second, the validation for refresh rates should ensure they're not just positive, but also have a reasonable maximum (like 3600 seconds) to prevent users from accidentally setting extremely long intervals. Third, you're validating in each plugin's `__init__`, but this means invalid configs only get caught when that specific plugin loads - instead, add a centralized validation method in the Config class that validates all plugin configurations at once during startup.

### Follow-up Prompt (Turn 3)

Good progress. However, the centralized validation method needs refinement. You're iterating through all sections, but you should only validate sections that correspond to actual plugins - check against `stats.getPluginsList()` to avoid false errors for non-plugin sections like `[global]` or `[outputs]`. Also, the validation rules you hardcoded should be defined as class attributes or constants in each plugin class (like `CONFIG_VALIDATION_RULES`) so they're maintainable and self-documenting. Finally, make sure the validation happens early enough in the startup sequence that it prevents Glances from starting with an invalid config, rather than failing partway through initialization.

### Follow-up Prompt (Turn 4)

Almost there. The validation rules as class attributes are much better, but you need to handle the case where a plugin doesn't define validation rules - currently it would raise an AttributeError. Use `getattr()` with a default empty dict. Also, I see you're logging warnings for invalid values but still allowing Glances to start - this should be configurable behavior. Add a `--strict-config` flag that makes validation errors fatal, while the default behavior falls back to safe defaults with warnings. Finally, add unit tests in `test_core.py` that verify the validation catches common misconfigurations like negative refresh rates, percentages over 100, and invalid threshold orderings.

---

## Prompt Set 2: Export Module Error Recovery

### Main Prompt (Turn 1)

Improve error handling and recovery in the export modules. Currently, if an export module (like InfluxDB, Prometheus, or Elasticsearch) encounters a connection error or export failure, it often silently fails or logs an error but doesn't attempt any recovery. Add a retry mechanism with exponential backoff for transient failures, and implement a circuit breaker pattern to temporarily disable exports that are consistently failing to prevent resource exhaustion and log spam.

### Follow-up Prompt (Turn 2)

The retry logic is a good start, but there are several issues. First, you're using `time.sleep()` for the backoff delays, which will block the entire Glances update cycle - this needs to be non-blocking. Track the next retry time and skip export attempts until that time is reached. Second, the circuit breaker state should be exposed through the plugin's stats so users can see which exports are currently failing in the UI. Third, you need to differentiate between retryable errors (network timeouts, temporary unavailability) and permanent errors (authentication failures, invalid configuration) - don't retry permanent errors.

### Follow-up Prompt (Turn 3)

Better, but the error classification logic is too simplistic. You're just checking exception types, but you need to inspect HTTP status codes for REST-based exports - 5xx errors are retryable, 4xx errors (except 429) are not. Also, the circuit breaker should have a half-open state where it periodically tests if the service has recovered, rather than staying permanently open after max failures. The state transitions need to be logged at INFO level so they appear in normal logs, not just DEBUG. Finally, add configuration options to `glances.conf` for retry behavior: `max_retries`, `initial_backoff_seconds`, and `circuit_breaker_threshold`.

### Follow-up Prompt (Turn 4)

Good improvements. However, you need to add proper cleanup in the export modules' `exit()` methods - if there are pending retries when Glances shuts down, they should be cancelled gracefully. The circuit breaker state should also be reset on Glances restart, not persisted. For the half-open state testing, make sure you're not testing too frequently - implement a minimum time in open state before attempting half-open (like 60 seconds). Also, the stats you're exposing should include not just current state, but also metrics like total_export_attempts, failed_exports, and last_success_timestamp. Add tests that verify the retry logic and circuit breaker state transitions.

---

## Prompt Set 3: Process Filtering Performance

### Main Prompt (Turn 1)

Optimize the process filtering performance in the processlist plugin. When users apply filters using the filter feature (GlancesFilter), the current implementation re-compiles regex patterns on every update cycle and applies them to every process. For systems with hundreds or thousands of processes, this creates unnecessary CPU overhead. Implement filter pattern caching and optimize the filtering algorithm to reduce the performance impact.

### Follow-up Prompt (Turn 2)

The regex caching helps, but you're still iterating through all processes for every filter check. Instead, you should compile all filter patterns into a single combined regex where possible, or at minimum short-circuit the evaluation when a process matches any filter. Also, I notice you're caching based on the filter string, but you're not invalidating the cache when the filter changes - you need to detect filter changes and clear the cache. The cache should also have a maximum size to prevent memory leaks if filters are changed frequently.

### Follow-up Prompt (Turn 3)

Good progress on the combined regex approach. However, you need to handle the case where filter_key is specified (like `username:nicolargo`) - these can't be combined into a single regex since they apply to different fields. Maintain separate code paths: one for simple name-based filters that can be combined, and one for key-specific filters. Also, the performance improvement should be measurable - add timing instrumentation that tracks how long filtering takes, and expose this in the debug output. Make sure the optimization doesn't change the filtering behavior - add tests that verify the same processes are filtered with and without the optimization.

### Follow-up Prompt (Turn 4)

The dual code path approach is working well. However, you need to add a benchmark to verify the performance improvement. Create a test in `test_perf.py` that generates a large mock process list (1000+ processes) and measures filtering time with various filter patterns. The optimized version should be at least 50% faster than the original. Also, I noticed you're not handling the case where regex compilation fails due to invalid user input - wrap the compilation in try-except and fall back to literal string matching if the regex is invalid. Finally, document the optimization in the code comments, explaining why the combined regex approach is faster.

---

## Prompt Set 4: Memory History Optimization

### Main Prompt (Turn 1)

Reduce memory consumption of the stats history feature. Currently, each plugin that has history enabled stores raw timestamp-value tuples in memory, and for systems running Glances for extended periods, this can consume significant memory. Implement a more memory-efficient history storage mechanism that uses downsampling for older data points while maintaining high resolution for recent data.

### Follow-up Prompt (Turn 2)

The downsampling approach is good, but you're applying it uniformly across all time ranges. Instead, implement a tiered approach: keep full resolution for the last hour, downsample to 1-minute averages for 1-24 hours ago, and downsample to 5-minute averages for older data. Also, you're creating new lists during downsampling which causes memory spikes - implement in-place downsampling that modifies the existing history structure. The downsampling should preserve min/max/avg values, not just averages, so users can still see peak values in historical data.

### Follow-up Prompt (Turn 3)

The tiered downsampling is much better, but there's an issue with the timestamp handling. When you downsample multiple points into one, you're using the first timestamp, but you should use the middle timestamp of the interval to accurately represent the time period. Also, the downsampling is happening synchronously during the update cycle - this could cause UI lag. Move the downsampling to a background thread that runs periodically (every 5 minutes) rather than on every update. Make sure the background thread properly handles the case where Glances is shutting down.

### Follow-up Prompt (Turn 4)

The background downsampling thread is working, but you need to add proper thread synchronization. The history data structure is being accessed by both the main update thread and the downsampling thread without locks, which could cause race conditions. Use `threading.Lock()` to protect history modifications. Also, add a configuration option `history_size` to `glances.conf` that lets users control how much history to keep - default to 24 hours but allow users to increase or decrease based on their needs. Add memory profiling tests that verify the new implementation uses significantly less memory than the original for long-running instances.

---

## Prompt Set 5: RESTful API Response Caching

### Main Prompt (Turn 1)

Add response caching to the RESTful API to reduce CPU usage when multiple clients are polling the same endpoints. Currently, every API request triggers a full stats serialization to JSON, even if the stats haven't changed since the last request. Implement a caching layer that serves cached responses when stats haven't been updated, with proper cache invalidation when stats are refreshed.

### Follow-up Prompt (Turn 2)

The basic caching works, but you're caching the entire response body as a string, which still requires JSON serialization on the first request after each update. Instead, cache the Python dict/list objects and only serialize to JSON when needed - this allows you to also support other formats (like MessagePack) without re-computing the data. Also, you're using a simple dict for the cache, but you need to add cache size limits and implement an LRU eviction policy to prevent unbounded memory growth if clients request many different endpoint combinations.

### Follow-up Prompt (Turn 3)

Good improvement with the object-level caching. However, the cache invalidation is too aggressive - you're invalidating the entire cache on every stats update, but some endpoints (like `/api/3/system`) return data that changes very infrequently. Implement per-endpoint cache invalidation based on which plugins were actually updated. Also, add ETags to the HTTP responses so clients can use conditional requests (If-None-Match) to avoid transferring data that hasn't changed. The ETag should be a hash of the cached response data.

### Follow-up Prompt (Turn 4)

The per-endpoint invalidation and ETags are working well. However, you need to add cache statistics that are exposed through a `/api/3/cache/stats` endpoint showing hit rate, miss rate, and current cache size - this helps users understand if caching is effective. Also, the cache should respect the `Cache-Control` headers - add support for `no-cache` and `max-age` directives. Make sure the caching doesn't break the existing API contract - add tests that verify all endpoints return the same data with and without caching enabled. Finally, add a `--disable-api-cache` command-line flag for debugging purposes.

---

## Prompt Set 6: Plugin Dependency Resolution

### Main Prompt (Turn 1)

Improve the plugin dependency resolution system. Currently, the DAG (Directed Acyclic Graph) in `glances/plugins/plugin/dag.py` defines plugin dependencies, but the dependency resolution happens at import time and doesn't handle circular dependencies gracefully. Refactor the dependency system to detect circular dependencies early and provide clear error messages, and ensure plugins are initialized in the correct dependency order.

### Follow-up Prompt (Turn 2)

The circular dependency detection is good, but the error message just lists the plugins involved - it should show the actual dependency chain that creates the cycle (e.g., "A depends on B depends on C depends on A"). Also, you're doing topological sort for initialization order, but you're not handling the case where a plugin's dependency is disabled by the user - the dependent plugin should either auto-disable with a warning, or the dependency should be marked as optional vs required. Add a way to specify optional dependencies in the DAG.

### Follow-up Prompt (Turn 3)

Better error messages and optional dependencies are good improvements. However, the initialization order is still determined globally at startup, but some plugins support dynamic enable/disable at runtime. When a plugin is disabled at runtime, you need to check if any enabled plugins depend on it and either disable them too or handle the missing dependency gracefully. Also, the dependency information should be exposed in the plugin's metadata so the UI can show users why a plugin is disabled (e.g., "Disabled: required dependency 'processcount' is not available").

### Follow-up Prompt (Turn 4)

The runtime dependency handling is working, but you need to add validation that the DAG structure itself is correct - currently if someone adds a dependency to a non-existent plugin, it silently fails. Add a validation function that runs during development (perhaps in the test suite) that verifies all dependencies reference real plugins. Also, the dependency resolution should be documented - add a section to the developer documentation explaining how to add dependencies when creating new plugins. Finally, add tests that verify the initialization order is correct for various dependency scenarios, including optional dependencies and disabled plugins.

---

## Prompt Set 7: SNMP Plugin Timeout Handling

### Main Prompt (Turn 1)

Improve timeout handling in the SNMP plugin. When monitoring remote systems via SNMP, network latency or unresponsive devices can cause the SNMP queries to hang, blocking the entire Glances update cycle. Implement proper timeout handling with configurable timeout values, and ensure that slow SNMP responses don't block other plugins from updating.

### Follow-up Prompt (Turn 2)

The timeout configuration is good, but you're still making SNMP calls synchronously in the main update thread. Even with timeouts, this means a slow SNMP device can delay the entire UI update. Move the SNMP queries to a background thread or use async I/O so they don't block the main update loop. The plugin should return the last known good values while waiting for the SNMP query to complete. Also, you need to handle the case where multiple SNMP OIDs are queried - they should be batched into a single SNMP request where possible to reduce network round-trips.

### Follow-up Prompt (Turn 3)

The async SNMP queries are working, but there's a race condition - if the SNMP query completes while the main thread is reading the stats, you could get inconsistent data (some old values, some new). Use proper locking or atomic updates to ensure stats are updated atomically. Also, the batched OID requests are good, but you need to handle partial failures - if some OIDs succeed and others timeout, you should update the successful ones rather than discarding all results. Add a status indicator to the stats showing the age of the data and whether the last update succeeded.

### Follow-up Prompt (Turn 4)

Good improvements on the atomic updates and partial failure handling. However, you need to add connection pooling for SNMP sessions - currently you're creating a new SNMP session for each query, which is inefficient. Reuse sessions across updates and only recreate them if they fail. Also, add SNMP v3 authentication support with proper credential handling - credentials should never be logged even in debug mode. The timeout values should be adaptive - if a device consistently responds quickly, reduce the timeout; if it's consistently slow, increase it within configured bounds. Add tests that simulate slow and unresponsive SNMP devices to verify the timeout handling works correctly.

---

## Prompt Set 8: Docker Container Metrics Enhancement

### Main Prompt (Turn 1)

Enhance the Docker container monitoring in the containers plugin to include network I/O statistics per container. Currently, the plugin shows CPU, memory, and disk I/O, but network statistics are missing even though Docker exposes this data through its API. Add per-container network bytes sent/received and packet counts to the container stats.

### Follow-up Prompt (Turn 2)

The network stats are being collected, but you're making a separate API call for each container's network stats, which is inefficient when monitoring many containers. The Docker API supports getting stats for all containers in a single streaming connection - refactor to use the streaming stats API. Also, the network stats you're showing are cumulative counters, but users typically want to see rates (bytes/sec). Calculate and display the rate by comparing with the previous update's values, similar to how disk I/O rates are calculated.

### Follow-up Prompt (Turn 3)

The streaming API and rate calculations are good, but there's an issue with containers that have multiple network interfaces - you're only showing stats for the first interface. Sum the stats across all interfaces to show total container network usage, but also provide a way to see per-interface breakdown in the detailed view. Also, the rate calculation fails on the first update because there's no previous value - handle this gracefully by showing 0 or N/A for the first update. Add proper error handling for containers that don't have network stats available (like containers in host network mode).

### Follow-up Prompt (Turn 4)

Good handling of multi-interface containers and edge cases. However, the network stats should respect the same hide/show configuration as other container stats - check the `hide` configuration option. Also, add unit conversion for network rates - show bytes/sec for low rates, KB/s for medium, MB/s for high, using the same `auto_unit()` function used elsewhere in Glances. The stats should be included in the JSON export and RESTful API responses. Finally, add tests that verify network stats are correctly calculated for containers with different network configurations (bridge, host, none).

---

## Prompt Set 9: Configuration File Hot Reload

### Main Prompt (Turn 1)

Add support for hot-reloading the configuration file without restarting Glances. Currently, if users modify `glances.conf`, they need to restart Glances for changes to take effect. Implement a file watcher that detects configuration file changes and reloads the configuration, applying changes to plugins that support runtime reconfiguration.

### Follow-up Prompt (Turn 2)

The file watching works, but you're reloading the entire configuration and reinitializing all plugins, which causes a noticeable pause in the UI and resets all stats history. Instead, implement selective reloading - compare the old and new configuration to determine which plugins are affected, and only reload those plugins. Also, some configuration changes can't be applied at runtime (like changing the server port) - detect these and log a warning that a restart is required. The reload should be atomic - if the new config file has syntax errors, keep using the old config rather than leaving Glances in a broken state.

### Follow-up Prompt (Turn 3)

The selective reloading is much better, but you need to handle the case where the config file is being edited - text editors often write to a temporary file and then rename it, which can trigger multiple file system events. Debounce the reload events so you only reload once after the file has stabilized (wait 1 second after the last change event). Also, the configuration validation you added earlier should run before applying the new config - if validation fails, keep the old config and show an error notification in the UI. Add a manual reload command (maybe a keyboard shortcut in the TUI) so users can trigger reload without modifying the file.

### Follow-up Prompt (Turn 4)

The debouncing and validation are working well. However, you need to add proper notification to users when a reload happens - in the TUI, show a brief message at the top of the screen; in the web UI, send a notification; in the API, include a `config_reload_timestamp` field. Also, the file watcher should handle the case where the config file is deleted and recreated - currently it loses the watch. Use a directory watcher instead of a file watcher to handle this. Add a `--disable-config-reload` flag for users who don't want this feature. Finally, add tests that verify configuration changes are correctly applied without restarting Glances, and that invalid configs are rejected.

---

## Prompt Set 10: Alert Action Script Templating

### Main Prompt (Turn 1)

Enhance the alert action script feature to support more flexible templating. Currently, action scripts receive a limited set of variables, and the templating is done with simple string replacement. Extend the templating system to use Jinja2 (which is already a dependency) and provide access to the full plugin stats context, not just the specific field that triggered the alert.

### Follow-up Prompt (Turn 2)

The Jinja2 templating is working, but you're passing the entire stats object to the template, which could expose sensitive information if users share their action scripts. Add a whitelist of safe fields that can be accessed in templates, and document which fields are available for each plugin. Also, the template rendering errors are being logged but the action script still executes with empty variables - instead, skip executing the action if template rendering fails, and make the error more visible (maybe add it to the alerts list). Add helper functions to the template context like `format_bytes()` and `format_time()` for common formatting needs.

### Follow-up Prompt (Turn 3)

The field whitelist and helper functions are good additions. However, you need to add sandboxing to the Jinja2 environment to prevent users from executing arbitrary Python code in templates - use `jinja2.sandbox.SandboxedEnvironment` instead of the regular `Environment`. Also, the template rendering should have a timeout to prevent infinite loops or very slow templates from blocking the alert system. Add template validation when the config is loaded - try to render each template with dummy data and report errors early rather than waiting for an alert to trigger.

### Follow-up Prompt (Turn 4)

The sandboxing and validation are important security improvements. However, you need to add better debugging support for action templates - when a template fails to render, the error message should show the template source and the line where the error occurred. Also, add a dry-run mode for testing action scripts (maybe a `--test-actions` flag) that triggers all configured actions with dummy alert data so users can verify their templates work correctly. The template context should include not just current values but also historical context (like previous value, trend direction). Finally, add examples of useful action templates to the documentation, like sending formatted Slack messages or creating detailed email alerts.

---

## Summary

These 10 prompt sets cover diverse aspects of the Glances codebase:

1. **Plugin Configuration Validation** - Input validation and error handling
2. **Export Module Error Recovery** - Resilience and retry logic
3. **Process Filtering Performance** - Optimization and caching
4. **Memory History Optimization** - Memory management and data structures
5. **RESTful API Response Caching** - Web API performance
6. **Plugin Dependency Resolution** - Architecture and initialization
7. **SNMP Plugin Timeout Handling** - Network I/O and async operations
8. **Docker Container Metrics Enhancement** - Feature addition and API integration
9. **Configuration File Hot Reload** - File system monitoring and dynamic updates
10. **Alert Action Script Templating** - Templating and security

Each prompt set:

- Defines a clear, realistic scope appropriate for a single PR
- Avoids prescribing implementation details in the first turn
- Provides progressively more specific technical guidance in follow-ups
- Stays within the original scope without scope creep
- Demonstrates expertise through specific, actionable feedback
- Requires production-ready code with proper error handling, testing, and documentation
