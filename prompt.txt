# Zev CLI - Code Human Preferences Prompts

This document contains 10 main prompts with 3 follow-up prompts each, designed for the Code Human Preferences CLI Edition project. All prompts adhere to the project guidelines for scope, complexity, and production-readiness iteration.

---

## Prompt Set 1: Command Validation and Sanitization

### Main Prompt (Turn 1)
The command history system currently saves all LLM responses without validating that the commands are actually executable or safe. Add validation logic that checks commands before they're saved to history to ensure they don't contain obvious syntax errors or malformed shell constructs.

### Follow-up Prompt (Turn 2)
The validation logic you added catches some basic issues, but it's using a simple string matching approach that could produce false positives. The validator should be more sophisticated - consider checking for balanced quotes, proper pipe usage, and valid command structure. Also, the error messages when validation fails aren't specific enough to help users understand what went wrong. Make the validation more robust and provide actionable error messages that explain exactly what's invalid.

### Follow-up Prompt (Turn 3)
I noticed the validation is happening after the LLM call in `save_options`, which means we're still making API calls even if the command would fail validation. The validation should happen earlier in the flow. Also, you've added validation tests, but they don't cover edge cases like commands with escaped characters, heredocs, or command substitution. Expand the test coverage to include these scenarios and ensure the validator handles them correctly without being overly restrictive.

### Follow-up Prompt (Turn 4)
The validation logic is working well, but there's a performance concern - you're running regex patterns on every command in a loop. For responses with multiple commands, this could be inefficient. Consider optimizing the validation to run patterns more efficiently. Additionally, the validation error messages are being printed directly to stdout, but they should integrate with the existing Rich console styling that the rest of the application uses. Make sure error output is consistent with the app's visual style.

---

## Prompt Set 2: Command Execution Feedback Loop

### Main Prompt (Turn 1)
When users select a command from the options, it gets copied to clipboard but there's no mechanism to track whether the command actually worked when they run it. Add a feedback mechanism that allows users to optionally report whether a command succeeded or failed after they run it, and store this feedback alongside the command in history.

### Follow-up Prompt (Turn 2)
The feedback prompt you added appears immediately after copying to clipboard, but this is poor UX because users haven't actually run the command yet. The feedback should be collected the next time the user runs zev, asking about the previously copied command. Also, the feedback data structure in `CommandHistoryEntry` needs a proper schema - you're storing feedback as a simple boolean, but we need to capture more context like timestamp and optional user notes about what went wrong.

### Follow-up Prompt (Turn 3)
Good progress on the deferred feedback collection. However, I see you're storing the "pending feedback" command in a separate file, which creates synchronization issues if the user runs multiple zev commands in succession. Instead, track pending feedback within the existing history file structure. Also, the feedback prompt is showing even when users run `zev --setup` or `zev --help`, which is annoying. The feedback prompt should only appear for actual command queries, not for utility flags.

### Follow-up Prompt (Turn 4)
The feedback collection is working, but there's no way for users to view or leverage this feedback data. Add a `--stats` flag that shows aggregated statistics about command success rates, most frequently used commands, and commands with the highest failure rates. The statistics should be displayed using Rich tables for good readability. Also ensure that the feedback data is properly anonymized if we ever add export functionality - don't include the actual prompts or commands in exported stats, only the metadata.

---

## Prompt Set 3: Multi-Command Workflow Support

### Main Prompt (Turn 1)
Users sometimes need to run a sequence of related commands to accomplish a task. Add support for the LLM to return multi-step workflows where commands should be executed in sequence, with each step having dependencies on previous steps completing successfully.

### Follow-up Prompt (Turn 2)
The workflow structure you've added to the response schema is good, but the UI for displaying multi-step workflows is confusing. When showing workflow steps, you're displaying them as separate command options, which makes it unclear that they're meant to be run in sequence. Design a better visual representation that clearly shows the step numbers, dependencies, and the overall workflow structure. Use Rich's tree or panel components to make the relationships clear.

### Follow-up Prompt (Turn 3)
The workflow display is much better, but there's a critical issue with how you're handling workflow execution. When a user selects a workflow, you're copying all commands to clipboard as a single multi-line string, but this doesn't work well with most terminal workflows. Instead, implement an interactive workflow executor that runs each step, shows the output, and asks for confirmation before proceeding to the next step. Users should be able to skip steps or abort the workflow at any point.

### Follow-up Prompt (Turn 4)
The interactive workflow executor is working, but it's not handling errors properly. If a command in the middle of a workflow fails (non-zero exit code), the executor should pause and ask the user whether to continue, retry, or abort. Also, the workflow execution isn't being saved to history correctly - you're only saving the final state, but we should save the entire execution trace including which steps succeeded, failed, or were skipped. This will help users debug failed workflows later.

---

## Prompt Set 4: Context-Aware Command Suggestions

### Main Prompt (Turn 1)
The current implementation sends basic OS and shell information as context, but it doesn't include information about the current directory contents or git repository state. Enhance the context gathering to include the current directory path, whether it's a git repo, and if so, the current branch and status summary.

### Follow-up Prompt (Turn 2)
The git context you're gathering is helpful, but you're calling `git status` synchronously in `get_env_context`, which blocks the UI if the git command is slow or hangs. Make the context gathering async or add a timeout so slow git operations don't freeze the application. Also, you're including the full git status output in the context, which can be very long for repos with many changes. Summarize the git status to just the essential information: branch name, number of modified files, and whether there are uncommitted changes.

### Follow-up Prompt (Turn 3)
Good work on the timeout handling. However, I noticed you're gathering git context even when the user's query has nothing to do with git (like "show disk usage"). This wastes time and adds unnecessary tokens to the LLM prompt. Implement smart context gathering that only includes git information when the query is likely git-related. You can use simple keyword matching on the user's query to determine relevance. Also, the directory path you're including shows the full absolute path, which could leak sensitive information in the prompt. Sanitize paths to show relative paths or just the directory name when appropriate.

### Follow-up Prompt (Turn 4)
The context relevance detection is working well, but there's an edge case: when users run `zev` with no arguments (interactive mode), you can't determine relevance until after they enter their query. Currently, you're gathering all context upfront in `run_no_prompt`, which means you're still gathering unnecessary context. Refactor the context gathering to happen after the user enters their query in interactive mode. Also, add unit tests for the context relevance detection logic to ensure it correctly identifies git-related queries without false negatives.

---

## Prompt Set 5: Command Explanation and Learning Mode

### Main Prompt (Turn 1)
Add a `--explain` flag that works with command queries to provide detailed explanations of what each suggested command does, breaking down the flags and arguments in an educational way. This should help users learn shell commands rather than just copying them blindly.

### Follow-up Prompt (Turn 2)
The explanation feature is helpful, but you're generating explanations by making a second LLM call after the initial command generation, which doubles the API cost and latency. Instead, modify the initial prompt to request detailed explanations alongside the commands when `--explain` mode is active. Update the `OptionsResponse` schema to include an optional detailed explanation field that's only populated in explain mode. Also, the explanations are being displayed as plain text, but they should use Rich formatting with syntax highlighting for command components.

### Follow-up Prompt (Turn 3)
The single-call approach is much better for performance. However, the explanations are sometimes too verbose and include information that's obvious to anyone familiar with basic shell usage. The prompt should instruct the LLM to focus explanations on non-obvious aspects, advanced flags, and potential gotchas. Also, when displaying explanations, you're showing them all at once before the user selects a command, which clutters the UI. Instead, show the brief explanation in the selection menu, and only display the detailed explanation after the user selects a command.

### Follow-up Prompt (Turn 4)
Much better UX flow. One issue: the detailed explanations don't include examples of the command's output or what the user should expect to see when they run it. Enhance the explanation prompt to request example outputs. Also, I noticed that in explain mode, the dangerous command warnings are being shown twice - once in the selection menu and again in the detailed explanation. Deduplicate this information. Finally, add a configuration option to make explain mode the default for users who want it always on, stored in the `.zevrc` file.

---

## Prompt Set 6: Command History Search and Filtering

### Main Prompt (Turn 1)
The `--recent` flag shows command history, but there's no way to search or filter through it. Add search functionality that allows users to filter history by keywords in the original query or in the returned commands.

### Follow-up Prompt (Turn 2)
The search functionality works, but you're implementing it as a separate `--search <term>` flag, which means users can't combine it with `--recent`. The UX would be better if search was integrated into the history display itself - when users select `--recent`, they should be presented with a search box at the top of the history list that filters results as they type. Use questionary's autocomplete or filtering features to make this interactive. Also, the search is case-sensitive, which is unnecessarily restrictive for this use case.

### Follow-up Prompt (Turn 3)
The interactive search is much better. However, the search only matches exact substrings, which means searching for "git commit" won't find "git commit -m message". Implement fuzzy matching or tokenized search where each word in the search query is matched independently. Also, you're loading the entire history file into memory and then filtering it, which could be slow for users with large history files. Consider implementing pagination or lazy loading for the history display.

### Follow-up Prompt (Turn 4)
The fuzzy search is working well. One issue: when users search and select a historical command, it shows the options from that historical query, but there's no indication of when that query was originally made. Add timestamps to `CommandHistoryEntry` and display them in the history list so users can see how old each entry is. Also, the search results aren't sorted in any meaningful way - they should be sorted by relevance score (based on match quality) and then by recency as a tiebreaker. Implement proper result ranking.

---

## Prompt Set 7: Provider-Specific Error Handling

### Main Prompt (Turn 1)
Different LLM providers have different error conditions and rate limits, but the current error handling is inconsistent across providers. Standardize error handling across all providers (OpenAI, Gemini, Ollama, Azure) to provide consistent, helpful error messages and implement retry logic with exponential backoff for transient failures.

### Follow-up Prompt (Turn 2)
The retry logic you added is using a simple fixed delay between retries, which isn't ideal for rate limit errors that often include a "retry-after" header. Implement proper exponential backoff with jitter, and respect retry-after headers when providers send them. Also, you're retrying all errors indiscriminately, but some errors like authentication failures shouldn't be retried. Categorize errors into retryable and non-retryable, and only retry the former.

### Follow-up Prompt (Turn 3)
Good progress on the retry categorization. However, the retry logic is duplicated across all four provider classes. Extract the retry logic into a shared decorator or base class method to eliminate duplication. Also, the error messages for Gemini provider are less helpful than the other providers - when a Gemini API call fails, you're just printing the HTTP error code, but you should parse the error response JSON and extract the actual error message like you do for OpenAI. Make error handling consistent across all providers.

### Follow-up Prompt (Turn 4)
The shared retry logic is much cleaner. One remaining issue: when retries are happening, there's no user feedback - the spinner just keeps spinning and users don't know if the app is frozen or retrying. Add retry progress indicators that inform users when retries are occurring and how many attempts remain. Use Rich's status updates to show messages like "Retrying... (attempt 2/3)". Also, add configuration options to `.zevrc` for max retry attempts and timeout values so power users can customize the retry behavior.

---

## Prompt Set 8: Command Safety Analysis Enhancement

### Main Prompt (Turn 1)
The current dangerous command detection relies entirely on the LLM marking commands as dangerous, but this isn't reliable. Implement client-side safety analysis that checks commands against a set of known dangerous patterns (like `rm -rf`, `dd`, `mkfs`, etc.) and warns users even if the LLM didn't flag the command as dangerous.

### Follow-up Prompt (Turn 2)
The pattern-based detection is working, but it's too aggressive - it's flagging commands like `rm -rf node_modules` which, while technically dangerous, are common and usually intentional. The safety analysis needs to be context-aware. For example, `rm -rf` in a project directory with `node_modules` is less concerning than `rm -rf /`. Implement heuristics that consider the command's target paths and context. Also, you're using simple string matching which can be bypassed with command substitution or variables. Make the pattern matching more robust.

### Follow-up Prompt (Turn 3)
The context-aware analysis is better, but there's a false negative issue: commands that use shell variables or command substitution aren't being analyzed properly because you're checking the literal command string. For example, `rm -rf $DIR` isn't flagged even if `$DIR` could be dangerous. While you can't evaluate variables at analysis time, you should flag commands that use variables in dangerous contexts and warn users to verify the variable values. Also, the dangerous command warnings are shown in red text, but they should be more prominent - consider using Rich panels or boxes to make them stand out more.

### Follow-up Prompt (Turn 4)
The variable detection is working well. However, the safety analysis is only running on commands that the LLM returns, not on commands from history when users use `--recent`. If a user selects a historical command that's now recognized as dangerous (maybe the pattern list was updated), they should still see the warning. Apply safety analysis consistently to all commands regardless of source. Also, add a `--safety-level` configuration option (low/medium/high) that controls how aggressive the safety checking is, and store this preference in `.zevrc`.

---

## Prompt Set 9: Response Caching and Offline Mode

### Main Prompt (Turn 1)
Users often ask similar or identical queries multiple times, which wastes API calls and costs money. Implement response caching that stores LLM responses locally and returns cached results for identical queries instead of making redundant API calls.

### Follow-up Prompt (Turn 2)
The caching is working for exact query matches, but it's too strict - "show disk usage" and "show disk usage?" are treated as different queries even though they're semantically identical. Normalize queries before caching by stripping trailing punctuation, converting to lowercase, and removing extra whitespace. Also, the cache has no expiration policy, which means it will grow indefinitely. Implement cache expiration based on age (e.g., cache entries older than 30 days are automatically removed) and size limits (e.g., keep only the 1000 most recent entries).

### Follow-up Prompt (Turn 3)
The cache normalization and expiration are working well. However, the cache is stored in a separate file from history, which creates data duplication - the same command responses are stored in both `.zevhistory` and the cache file. Consolidate these by using the history file as the cache source. When a query matches a recent history entry, return that result instead of making a new API call. Also, you're not providing any feedback to users when a cached result is returned - they should know they're seeing a cached response. Add a subtle indicator like "(cached)" in the status message.

### Follow-up Prompt (Turn 4)
Good consolidation of cache and history. One issue: the cache matching is still based on exact query text matching, which misses semantically similar queries like "list python processes" vs "show running python processes". While implementing full semantic similarity is complex, you could at least implement simple synonym handling for common command verbs (list/show/display, find/search/locate, etc.). Also, add a `--no-cache` flag that forces a fresh API call even if a cached result exists, for cases where users want the most up-to-date response.

---

## Prompt Set 10: Configuration Validation and Migration

### Main Prompt (Turn 1)
The configuration system currently doesn't validate the config file format or values. If users manually edit `.zevrc` and introduce errors, the app fails with cryptic error messages. Add configuration validation that checks for required fields, valid provider names, and properly formatted API keys when the config is loaded.

### Follow-up Prompt (Turn 2)
The validation you added runs when the config is loaded, but it's using assertions which cause the app to crash with stack traces when validation fails. Instead, validation errors should be caught and presented to users with helpful error messages that explain what's wrong and how to fix it. For example, if the API key format is invalid, suggest running `zev --setup` to reconfigure. Also, you're validating all provider configs even though only one provider is active - only validate the configuration for the currently selected provider.

### Follow-up Prompt (Turn 3)
Much better error handling. However, there's no validation for the API keys themselves - you're just checking that they're non-empty strings. Different providers have different API key formats (OpenAI keys start with "sk-", Gemini keys have specific lengths, etc.). Add format validation for each provider's API keys to catch obvious errors before making API calls. Also, when validation fails, you're suggesting users run `--setup`, but if they just made a typo in the config file, it would be faster to tell them exactly which line is wrong so they can fix it manually.

### Follow-up Prompt (Turn 4)
The API key format validation is helpful. One more issue: when the config file format changes between versions (like when new providers are added), users with old config files might be missing required fields. Implement configuration migration that automatically updates old config files to the new format, adding default values for new fields. Also, add a `--validate-config` flag that checks the configuration without running a query, so users can verify their config is valid after manual edits. The validation output should clearly indicate what's correct and what needs fixing.

---

## Notes on Prompt Design

All prompts in this document follow these principles:

1. **Appropriate Scope**: Each main prompt defines a single, atomic feature that could reasonably be implemented in one PR
2. **User Perspective**: Prompts describe what needs to be done without prescribing implementation details
3. **Reasonable Challenge**: None of the prompts should be completable in a single turn - they require iteration
4. **No Explicit PR-Ready Instructions**: Prompts don't tell the model to make changes "production ready"
5. **Progressive Refinement**: Follow-up prompts provide specific, expert-level feedback on the implementation
6. **No Scope Creep**: Follow-ups stay within the original scope and guide toward production quality
7. **Specific and Prescriptive**: Follow-ups demonstrate expertise by pointing out specific issues and suggesting concrete improvements
8. **Unique to Context**: Follow-ups address issues specific to the implementation, not generic advice

These prompts are designed to test the model's ability to:
- Implement features correctly and completely
- Handle edge cases and error conditions
- Write clean, maintainable code matching the codebase style
- Add appropriate tests and documentation
- Self-review and iterate toward production quality
